# 并发算力限制优化方案设计

## 1. 背景与问题描述

### 1.1 当前实现（按挂钟时间计费）
目前，`nvshare` 是根据“挂钟时间”（Wall-Clock Time，即物理流逝时间）来计算计算资源使用的。
- 如果一个客户端从 `T0` 到 `T1` 占用了 GPU 锁，它的使用量就是 `T1 - T0`。
- 这个计算与当时有多少其他客户端同时运行无关。

### 1.2 并发模式下的问题
当调度器允许通过“时间重叠”（Overlap）机制同时运行多个客户端时，严格的挂钟时间计费会导致“过早限流”（Premature Throttling）和低 GPU 利用率。

#### 场景举例:
- **窗口大小**: 2000ms
- **客户端 A**: 限制 50% (配额: 1000ms)
- **客户端 B**: 限制 50% (配额: 1000ms)
- **模式**: 并发模式 (两者都在 T=0 时刻开始)

#### 执行流程 (当前状况):
1. **T=0ms**: A 和 B 同时启动。
2. **T=1000ms**:
   - A 已经运行了 1000ms (挂钟时间)，达到上限。**A 被限流**。
   - B 已经运行了 1000ms (挂钟时间)，达到上限。**B 被限流**。
3. **T=1000ms ~ 2000ms**:
   - A 和 B 都被限流中。
   - GPU 处于 **空闲状态**。

**结果**:
- 配置的总限制: 100% (50% + 50%)
- 实际 GPU 利用率: 50% (0-1s 活跃, 1-2s 空闲)
- **偏差**: 用户期望 50% + 50% 能大致填满 GPU (接近 100% 利用率)，但实际上只得到了 50%。

---

## 2. 根本原因分析 (Root Cause Analysis)

目前“算力限制”的定义是 *“客户端在窗口期内被允许持有 GPU 锁的时长”*。

在 **串行模式 (Serial Mode)** 下，这与 GPU 资源是 1:1 对应的。
在 **并发模式 (Concurrent Mode)** 下，如果多个任务共享 GPU，这个定义就有缺陷了。因为多个任务共享 GPU 时，单位时间内每个任务消耗的“独占资源”实际上是少于 100% 的。

如果 2 个任务共享 GPU，它们实际上是在“分摊”计算能力（假设 GPU 硬件通过时间片或空间共享来调度它们）。因此，如果对这段“共享时间”仍然按“独占时间”的全价进行计费，就是严重的“过度计费”。

---

## 3. 建议方案：加权计费 (Weighted Billing)

为了获得准确的总利用率，我们在计费时必须考虑并发度。在 GPU 上运行的“成本”应该与并发任务的数量成反比。

**公式**:
`计费时长 = 挂钟时长 / 当前并发客户端数量`

### 3.1 加权计费下的执行推演

#### 相同场景 (A=50%, B=50%, 窗口=2000ms)
1. **T=0ms**: A 和 B 同时启动。`n_running = 2` (并发数为2)。
2. **T=1000ms**:
   - 挂钟流逝: 1000ms。
   - A 的计费消耗: `1000 / 2 = 500ms`。 (剩余配额: 500ms)
   - B 的计费消耗: `1000 / 2 = 500ms`。 (剩余配额: 500ms)
   - **结果**: 都没有达到上限，继续运行。
3. **T=2000ms**:
   - 挂钟流逝: 2000ms (自开始)。
   - 新增挂钟流逝: 1000ms。
   - A 新增计费: `1000 / 2 = 500ms`。总计: 1000ms。 **A 此时被限流**。
   - B 新增计费: `1000 / 2 = 500ms`。总计: 1000ms。 **B 此时被限流**。
4. **窗口结束**:
   - GPU 从 0ms 到 2000ms 一直处于活跃状态。
   - **利用率**: 100%。

### 3.2 动态并发处理
如果在运行过程中 `n_running` 发生变化，计费也会随之动态调整。

#### 场景: A (限制 50%), B (限制 20%)
1. **T=0 ~ T=800 (并发 n=2)**:
   - 挂钟: 800ms。
   - A 计费: 400ms。
   - B 计费: 400ms (B 的总限额是 2000ms * 20% = 400ms)。 **B 此时耗尽配额被限流**。
2. **T=800 ~ T=2000 (仅 A 运行, n=1)**:
   - B 停止。A 继续单独运行。
   - 挂钟流逝: 1200ms。
   - A 计费: `1200 / 1 = 1200ms`。
   - A 总计: 400 + 1200 = 1600ms。
   - A 限额: 1000ms。
   - 可以在此推算出 A 会在何时被限流。

   **重新计算 A 的限流点**:
   - T=800 时，A 已用掉 400ms。
   - A 剩余配额 600ms。
   - 接下来单独运行 (费率 1.0)，A 还能跑 600ms。
   - A 的限流时刻 = 800 + 600 = 1400ms。
   - GPU 总活跃时间: 0~1400ms (70% 利用率)。
   - 预期: 50% + 20% = 70%。 **完美符合预期。**

---

## 4. 实现细节建议

### 4.1 数据结构
不需要修改 `struct nvshare_client` 数据结构。我们只需要修改计算逻辑。

### 4.2 辅助函数
需要一个辅助函数来高效获取当前正在运行的客户端数量。
```c
static int count_running_clients(struct gpu_context* ctx) {
  int count = 0;
  LL_FOREACH(ctx->running_list, req) count++;
  return count > 0 ? count : 1;
}
```

### 4.3 逻辑植入点

#### A. 动态检查 (`timer_thr_fn`)
在定时器循环内，计算 `current_usage` 时应用权重：
```c
int n = count_running_clients(ctx);
long pending_wall_time = now_ms - c->current_run_start_ms;
long pending_billed_time = pending_wall_time / n; // 权衡计算
long current_total_usage = c->run_time_in_window_ms + pending_billed_time;
```

#### B. 最终结算 (`remove_req`)
当客户端停止或被移除时进行结算：
```c
int n = count_running_clients(ctx); /* 使用当前的并发数进行近似 */
long duration = now_ms - c->current_run_start_ms;
c->run_time_in_window_ms += (duration / n);
```

### 4.4 关于近似误差的说明
严格来说，`n_running` 可能会在运行期间发生变化。在 `remove_req` 或 `timer` 检查时只使用“当前时刻”的 `n` 值是一种近似算法（黎曼和）。
但由于 `timer_thr_fn` 运行频率很高（即使睡眠也是基于动态计算），且 `remove_req` 是事件驱动的（只有在开始/停止时并发数才会变），这种分段线性近似在数学上对我们的需求来说已经足够精确了。

---

## 5. 方案收益总结
1. **利用率准确**: 50% + 50% = 100%，不再是 50%。
2. **公平性**: 共享 GPU 的客户端因为获得的性能并非独占，所以支付的“时间成本”也相应降低，这更公平。
3. **无需配置变更**: 仍然兼容现有的百分比注解 (`nvshare.com/gpu-core-limit`)，只需升级调度器逻辑。

---

## 6. 方案评审与对比分析 (Opus Review)

### 6.1 加权计费方案可行性分析

#### 优点

1. **思路精巧**：通过改变"计费规则"而非"调度规则"来解决问题，改动集中且风险低。

2. **数学上正确**：
   - 场景验证：A(50%) + B(50%) 并发运行
   - 加权计费下：2000ms 窗口内，每个任务消耗 `2000/2 = 1000ms` 计费时间
   - 实际 GPU 运行 2000ms = 100% 利用率 ✓

3. **动态适应**：当并发数变化时，计费自动调整，无需额外协调逻辑。

4. **实现简洁**：只需修改两处计费逻辑（`timer_thr_fn` 和 `remove_req`），约 20-30 行代码。

#### 潜在问题与风险

1. **语义变化**：
   - 原语义：`50%` = "允许使用 50% 的时间"
   - 新语义：`50%` = "允许消耗 50% 的计费单位"（实际时间取决于并发数）
   - **影响**：用户可能困惑为何设置 50% 但任务运行了 2000ms

2. **超配场景**：
   - 场景：A(80%) + B(80%) = 160%
   - 加权计费下：两者可同时运行 `2000 * 0.8 * 2 = 3200ms` 的挂钟时间
   - 但窗口只有 2000ms，所以两者都不会被限流
   - **结果**：实际行为等同于无限制并发（用户设置的 80% 上限失效）

3. **竞争不公平**：
   - 场景：A(50%) 在 T=0 启动，B(50%) 在 T=500 加入
   - A 在 0-500ms 独占时消耗 `500/1 = 500ms` 计费
   - A 在 500-1500ms 并发时消耗 `1000/2 = 500ms` 计费，总计 1000ms，被限流
   - B 在 500-1500ms 消耗 `1000/2 = 500ms`，还剩 500ms 配额
   - B 在 1500-2000ms 独占消耗 500ms，正好用完
   - **结果**：A 运行 1500ms，B 运行 1500ms，但 A 被"早到惩罚"

4. **GPU 实际行为不符合假设**：
   - 加权计费假设：2 个任务并发时，每个获得 50% GPU 算力
   - 实际情况：GPU 调度是黑盒，取决于 CUDA kernel 特性、SM 占用率等
   - 计算密集型任务可能争抢严重（<50%），IO 密集型可能重叠良好（>50%）

### 6.2 与 Opus 方案对比

| 维度 | 加权计费 (Gemini) | 协调串行 (Opus-1) | 动态并发 (Opus-2) | 公平轮转 (Opus-3) |
|------|------------------|------------------|------------------|------------------|
| **核心思路** | 改变计费规则 | 改变调度时间片 | 改变并发策略 | 细粒度轮转 |
| **代码改动量** | ~30行 | ~100行 | ~30行 | ~200行 |
| **语义一致性** | ⚠️ 语义变化 | ✓ 保持原语义 | ✓ 保持原语义 | ✓ 保持原语义 |
| **超配处理** | ❌ 限制失效 | ✓ 按比例缩放 | ✓ 强制串行 | ✓ 按比例分配 |
| **公平性** | ⚠️ 先到惩罚 | ✓ 按配额公平 | ⚠️ 先到先服务 | ✓✓ 最公平 |
| **切换开销** | 最低 | 中 | 低 | 高 |
| **实现复杂度** | 低 | 中 | 低 | 高 |

### 6.3 场景对比分析

#### 场景 1: A(50%) + B(50%) = 100%

| 方案 | A 运行时间 | B 运行时间 | GPU 利用率 |
|------|-----------|-----------|-----------|
| 当前实现 | 1000ms | 1000ms (重叠) | 50% |
| 加权计费 | 2000ms | 2000ms (重叠) | 100% |
| 协调串行 | 1000ms | 1000ms (串行) | 100% |
| 动态并发 | 2000ms | 2000ms (重叠) | 100% |

**结论**：配额总和=100%时，所有优化方案效果相同。

---

#### 场景 2: A(50%) + B(60%) = 110%

| 方案 | A 运行时间 | B 运行时间 | GPU 利用率 | 备注 |
|------|-----------|-----------|-----------|------|
| 当前实现 | 1000ms | 1200ms (重叠) | ~60% | 重叠后同时空闲 |
| 加权计费 | 2000ms | 2000ms (重叠) | 100% | ⚠️ 50%/60%限制失效 |
| 协调串行 | 909ms | 1091ms (串行) | 100% | 按比例缩放配额 |
| 动态并发 | 1000ms | 1200ms (串行) | 100% | 先A后B串行 |

**结论**：加权计费在超配场景下会使限制失效，其他方案保持限制有效。

---

#### 场景 3: A(30%) + B(30%) + C(30%) = 90%

| 方案 | GPU 利用率 | 行为 |
|------|-----------|------|
| 当前实现 | 30% | 三者同时运行600ms后全部空闲 |
| 加权计费 | 90% | 三者运行至窗口末尾 |
| 协调串行 | 90% | 串行运行600ms * 3 |
| 动态并发 | 90% | 并发运行（配额<100%） |

**结论**：配额总和<100%时，加权计费和动态并发等效。

### 6.4 综合评估与推荐

```
推荐优先级：
1. 加权计费 (Gemini)  ★★★★☆  - 如果接受语义变化和超配风险
2. 动态并发 (Opus-2)  ★★★★☆  - 如果需要严格保持配额语义
3. 协调串行 (Opus-1)  ★★★☆☆  - 如果需要按比例缩放配额
4. 公平轮转 (Opus-3)  ★★☆☆☆  - 除非有极高公平性要求
```

### 6.5 建议的混合方案

结合两者优点，可考虑**加权计费 + 超配保护**：

```c
static long calculate_weighted_usage(struct gpu_context* ctx, 
                                      struct nvshare_client* c,
                                      long wall_time) {
    int n = count_running_clients(ctx);
    int total_quota = calculate_total_quota(ctx);
    
    if (total_quota <= 100) {
        /* 配额总和 ≤ 100%：使用加权计费 */
        return wall_time / n;
    } else {
        /* 配额总和 > 100%：使用原始计费（触发串行调度） */
        return wall_time;
    }
}
```

这样既能在合理配置下获得加权计费的好处，又能在超配时保持限制有效。
