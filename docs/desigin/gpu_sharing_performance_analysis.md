# GPU 共享性能与显存行为分析 (Updated)

本文档针对用户在测试 `nvshare` 时提出的两个关键疑问进行深度技术分析，特别是针对性能严重下降 (95%) 的根本原因。

## 1. 为什么 `nvidia-smi` 显存占用低？(已解答)

简而言之：**按需分页 (Demand Paging)**。
- 虽然分配了 12GB，但 PyTorch 每次迭代只流式处理数据。
- `nvidia-smi` 显示的 1GB 是 "驻留集 (Resident Set)"，即当前 GPU 正在处理的数据块大小。
- 实际上，数据正在通过 PCIe 总线在内存和显存间高速流动（Streaming）。这解释了为什么即使在单任务 (gpu-2) 情况下，速度也是 1.5s/it（受限于 PCIe 带宽），而不是纯显存计算的毫秒级速度。

## 2. 为什么性能从 1.5s/it 下降到 35s/it (下降 95%)？

这是一个非常敏锐的观察。单纯的 "切换开销占比 30%" 确实无法解释 20 倍的性能下降。
**根本原因在于 `nvshare` 的动态流控机制 (Pending Kernel Window) 陷入了"死亡螺旋"。**

### 机制分析
`nvshare` 为了防止某个进程提交过多 Kernel 导致 GPU 锁无法释放，引入了一个流控窗口 (`Pending Kernel Window`)：
- **正常模式**: 窗口逐步增大（如 gpu-2 增长到 64）。这意味着 Python 端一次性“发射” 64 个 Kernel，GPU 异步执行，流水线被填满，PCIe 传输和计算重叠，性能最高 (1.5s/it)。
- **保护触发**: 代码中设定如果一次 `cudaDeviceSynchronize` 耗时超过 **10秒**，系统会判定GPU严重拥堵，将窗口**强制降为 1**。

### 故障复盘
1. **触发点**: 当任务 4 正在运行且需要进行上下文切换时，或者刚获得锁开始运行时，由于系统正处于严重的显存抖动（Thrashing）状态，驱动程序需要处理复杂的页表更新和内存搬运。
2. **超时**:某次同步操作受到这些系统开销的干扰，耗时超过了 10 秒（这在已分配超大 Unified Memory 的情况下容易发生）。
3. **降级**: `nvshare` 检测到超时，误判为恶意占用，将 `Pending Kernel Window` 重置为 **1**。
4. **性能崩溃**:
    - 窗口为 1 意味着：Python 发射 1 个 Kernel -> 等待 GPU 执行完 -> 再发射下一个。
    - **流水线彻底断裂**。
    - 此时，原本被掩盖的 CPU-GPU 通信延迟、Python 开销、PCIe 握手延迟全部暴露出来，并且还要叠加 Unified Memory 的页缺失开销。
    - 这就是为什么速度变成了极慢的 35s/it（完全同步执行模式）。

### 关于 "1GB 显存置换需要 10-20秒？"
- 纯粹拷贝 1GB 数据通常只需要 <0.1秒 (PCIe Gen4)。
- 但是，这里的 "10-20秒" 并不是单纯的数据拷贝时间，而是：
  - **Unified Memory 管理开销**: 驱动程序需要扫描 12GB 的虚拟地址空间，处理页表，驱逐旧页面。在多进程高争用下，驱动内部锁竞争可能导致显著延迟。
  - **排队延迟**: 如果系统负载极高，CUDA 命令队列本身可能有积压。

## 解决方案

虽然我们找到了根因（窗口降级），但在不修改代码逻辑（放宽 10s 限制）的前提下，最有效的规避手段依然是 **减少切换频率**。

减少切换频率（增大时间片）可以：
1. 让任务有足够的时间预热，让窗口值 (`Pending Kernel Window`) 有机会重新增长到 64。
2. 减少触发 "10秒卡顿" 的概率（大部分卡顿发生在切换前后的不稳定期）。

因此，我们坚决推荐：
**配置 `NVSHARE_SWITCH_TIME_MODE=fixed` 且 `VALUE=600` (10分钟)**，让大显存任务尽可能一口气跑完，避免触发流控降级。
