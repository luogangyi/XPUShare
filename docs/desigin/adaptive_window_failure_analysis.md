# 自适应流控窗口机制效果分析

## 结论
机制**部分生效**，但在极端显存超卖（3x Oversubscription + 12GB/Pod）场景下，**未能挽救性能**。

主要原因在于 **Warm-up 逻辑缺陷** 以及 **物理带宽瓶颈导致的 Live Lock**。

## 详细分析

### 1. 机制是否生效？
**是，代码逻辑被触发了，但并未产生预期收益。**

*   **Warm-up 机制**: 日志显示 `warmup=1` 确实出现，说明预热状态被正确置位。
*   **AIMD 机制**: 日志 `[WARN]: Critical timeout (26 s). AIMD reduced window to 4` 证明系统检测到了拥塞，并正确执行了窗口缩减（降级）操作。

### 2. 为什么还出现大量切换？
**“切换”是调度器的行为，流控窗口无法阻止。**
流控窗口（Client端）只是控制 **“只有这一个Pod在跑时，我发多快”**。而 `DROP_LOCK` 是 Server 端强制剥夺执行权。
*   当 Pod 发生 26s 的卡顿时，调度计时器仍在走。
*   如果调度器的时间片（Time Slice）是 30s，Pod 花了 26s 搬运内存，也就只剩下 4s 运行时间。一旦时间到，调度器必然强制切换 (`DROP_LOCK`)，导致刚搬进来的热数据又要被淘汰。

### 3. 为什么性能没提上来？（根本原因分析）

#### A. 设计缺陷：Warm-up 检查时机滞后
当前代码逻辑：
```c
// 先做耗时的同步 (可能耗时 26s)
result = real_cuCtxSynchronize(); 
...
// 然后检查是否还在预热期
time_t now = time(NULL);
if ((now - lock_acquire_time) < warmup_period) { ... }
```
**问题**: 如果 `lock_acquire_time` 是 0s，预热期设定 30s。
1. 若 Pod 开始同步时是 5s（处于预热期）。
2. 同步因为缺页极其严重，耗时 **26s**。
3. 同步结束时是 31s。
4. 此时检查 `now(31) - lock(0) > 30` -> **判定预热失效**。
5. 进入 `else` 分支 -> 触发 `Critical Timeout` 惩罚 -> 窗口骤降。

**后果**: 本该豁免这次“大换页”的保护机制，因为换页时间太长把保护期耗尽了，导致 Pod 在刚换页完成准备起飞时，被一棒子打回原型（Window降级）。

#### B. 物理瓶颈导致的死锁 (Thrashing Live Lock)
即便窗口机制完美工作（比如一直保持 512），性能也未必能救回。
*   **物理常数**: 搬运 12GB 数据 @ PCIe 3.0/4.0 x16 在拥塞下可能就是需要 20-26秒。
*   **时间片冲突**: 调度器时间片 (Time Quantum, TQ) 若设置得不够长（例如 45s 或 60s），Pod 将永远处于 **"获得锁 -> 搬运数据(26s) -> 刚准备算 -> 时间到被抢占"** 的死循环中。
*   **现象**: 日志中大量的 `Early release timer elapsed but we are not idle` 和 `Pending Kernel Window` 低值徘徊，印证了 Pod 一直在等待数据，而非计算。

## 改进建议

1.  **修复代码逻辑**: 在 `cuCtxSynchronize` **之前** 记录是否处于 warm-up 状态，或者在该次判断时豁免掉本次 sync 的耗时。
2.  **延长预热期**: 对于大显存应用（12GB+），30s 的预热期明显不足（一次换页就耗光了）。建议配合 `NVSHARE_KERN_WARMUP_PERIOD_SEC=60` 或更长。
3.  **调度策略调整**: 必须在 Server 端配合 **动态时间片** 或 **最小执行保证**。检测到 Memory Thrashing 时，要么延长该 Pod 的时间片至 2分钟，要么禁止切换（直到它完成一定量的计算）。
