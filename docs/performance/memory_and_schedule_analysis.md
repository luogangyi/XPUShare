# 显存占用差异与并发性能瓶颈深度复盘报告

## 1. 核心结论摘要

针对您观察到的两个核心现象，经深度分析代码与CUDA机制，结论如下：

1.  **单任务显存 "1G vs 12G" 现象**：这是 **Unified Memory (UVM) 的统计假象**。`nvidia-smi` 仅统计物理驻留 (Resident) 显存，而在无竞争环境下，驱动可能采用 Lazy Allocation 或未将其计入常规 Process Memory，但**实际上数据必须在 GPU 上**才能维持与 Native 一致的计算速度。这证明不存在物理带宽瓶颈。
2.  **并发性能 "3%" 现象**：之前的分析方向（调整窗口、抢占策略）**未触及根本原因**。根本瓶颈并非 "调度粒度" 或 "显存分配量"，而是 **按需分页 (Demand Paging) 带来的延迟风暴**。
    -   **关键数据**：恢复 12GB 数据，若用缺页中断方式（现状），耗时约 **47秒** (延迟敏感)；若用 DMA 预取方式（未实现），耗时仅 **0.75秒** (带宽敏感)。
    -   **失败原因**：在未实现预取的情况下，无论如何调整窗口大小时，只要发生任务切换，就需要 50秒来 "热身"。若时间片小于此值，任务将永远处于由于缺页导致的 "假死" 状态，有效产出接近于零。

---

## 2. 深度分析：问题一 (单任务显存差异)

**用户疑问**：为何 `nvidia-smi` 显示 nvshare 仅占 1G (Native 12G)，但速度一样快 (159s vs 155s)？

### 2.1 机制解密
- **Native (`tests/pytorch-add.py`)**：直接调用 `cudaMalloc`。这是 **显式物理显存分配**。分配即占用，`nvidia-smi` 忠实记录 12GB。
- **Nvshare**：`hook.c` 拦截分配并强制替换为 `cuMemAllocManaged` (Unified Memory)。这是 **虚拟显存分配**。
    - **Lazy Resident**: 在 UVM 机制下，`nvidia-smi` 报告的 "Memory Usage" 通常指 **Resident Memory** (当前硬驻留在 VRAM 的页)。
    - **执行时刻**：当 PyTorch 执行 `torch.add(x, y)` 的 4000 次循环时，GPU 必须访问 x, y, z 的所有数据。显存带宽高达几百 GB/s。如果数据真在 Host 内存 (System RAM)，受限于 PCIe (16GB/s)，速度会慢 10-20 倍。
    - **结论**：既然速度持平 (159s ≈ 155s)，说明 **在计算发生的毫秒级瞬间，12GB 数据绝对完整的驻留在 GPU VRAM 中**。`nvidia-smi` 显示的 "1G" 极大概率是驱动层对 UVM 内存的统计归类差异（例如未计入专用计算显存，或采样时刻处于非活跃状态），而非物理事实。

---

## 3. 深度分析：问题二 (并发性能崩溃)

**用户疑问**：为何并发运行时速度只有 3%？为何调整窗口、策略均无效？

### 3.1 之前的分析为何失效？
您尝试了：
- *修改 Kernel Window 增长速度* -> 试图让任务更快进入 "全速状态"。
- *修改切换策略* -> 试图减少切换频率。
- *修改显存分配* -> 试图减少总量。

这些尝试都基于一个假设：**"只要给任务足够的时间，它就能跑起来"。**
但在 UVM 机制下，这个假设有一个巨大的**门槛** —— **数据迁移延迟**。

### 3.2 真正的罪魁祸首：缺页中断风暴 (Page Fault Storm)
当 nvshare 调度器 (`scheduler.c`) 执行切换，向 Client 发送 `LOCK_OK` 时，Client 的显存处于 "冷" 状态 (在 Host RAM)。

#### 两种数据恢复方式对比：

| 方式 | 机制 | 计算公式 | 耗时 (12GB) | 当前状态 |
| :--- | :--- | :--- | :--- | :--- |
| **A. 按需分页 (Demand Paging)** | GPU 访问页 -> 触发中断 -> CPU 处理 -> 搬运 4KB -> 恢复 | 3,145,728 Pages × ~15µs | **~47.2 秒** | **当前 nvshare 实现** |
| **B. 批量预取 (Bulk Prefetch)** | 发送 DMA 指令 -> GPU Copy Engine 满带宽搬运 | 12GB / 16GBps (PCIe) | **~0.75 秒** | **未实现** |

### 3.3 场景复现 (为何是 3%)
1.  **切换发生**：Task B 获得锁。
2.  **执行开始**：Task B 试图计算。由于数据不在 VRAM，触发缺页中断。
3.  **陷入泥潭**：GPU 核心挂起，等待 CPU 和 OS 处理中断。每秒只能处理约 6万次中断 (66k IOPS)，而我们需要处理 300万次。
4.  **时间片耗尽**：假设时间片 (TQ) 为 30秒 (默认)。
    - 在 30秒内，Task B 仅拼命搬运了约 60% 的数据 (30s / 47s)。
    - **有效计算量 = 0**。
5.  **被迫让出**：调度器发送 `DROP_LOCK`。Task B 释放 GPU，刚刚搬运进来的热数据随后被 Task C 挤出。
6.  **恶性循环**：下一轮 Task B 再次获得锁时，一切从头开始。

**3% 的来源**：偶尔有极少量的重叠或刚好某些页未被驱逐，勉强推进了一点点进度。实际上系统处于 **Thrashing (颠簸)** 状态——99% 的时间在处理中断和搬运数据，1% 的时间在计算。

---

## 4. 解决方案

之前的调整（窗口大小等）相当于在 100米的赛道上调整跑鞋，但面前却有一堵 50米厚的墙（47秒延迟）。

### 唯一有效解：实现 `prefetch_all_allocations`

必须修改 `src/hook.c`，在获得锁的瞬间 (`LOCK_OK` 处理逻辑中)，**主动**将所有显存块预取到 GPU。

```c
// 伪代码逻辑
void on_lock_acquired() {
    cudaStream_t stream;
    cudaStreamCreate(&stream);
    // 遍历所有分配的内存块
    LL_FOREACH(allocation_list, alloc) {
        // 使用 DMA 引擎批量搬运，绕过 Page Fault
        cuMemPrefetchAsync(alloc->ptr, alloc->size, current_device, stream);
    }
    cudaStreamSynchronize(stream);
    // 此时数据已就绪，计算可以全速通过
}
```

### 验证方法
若实施上述预取方案，预期：
1.  **切换耗时**：从 "无底洞" 变为 固定的 ~1-2秒 (12GB 数据传输时间)。
2.  **性能恢复**：任务将在 2秒后进入全速计算状态。即便 30秒切一次，也能以此获得约 28秒的有效计算时间 (效率 >90%)。
